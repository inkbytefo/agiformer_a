# AGIFORMER T4 GPU Ultra-Optimized Configuration
# Maximum performance for Tesla T4 with 16GB VRAM
# Memory-optimized with chunked logic expert processing

# Model Architecture - T4 Ultra Optimized
model:
  vocab_size: 256  # Character vocabulary
  d_model: 384    # Further reduced for T4
  n_layers: 6     # Minimal but effective
  n_heads: 6      # Reduced for T4
  d_ff: 1536      # Reduced for T4

  # Mixture of Experts - Minimal for T4
  n_experts: 1    # Single expert for maximum speed
  expert_types: ["language"]  # Only language expert

  # Memory System - Minimal for T4
  memory_size: 500   # Minimal memory
  max_seq_len: 256   # Short sequences for T4

  # Features - Minimal configuration
  use_linear_attention: false
  use_memory: false     # Disabled to save memory
  use_introspection: false  # Disabled for speed
  use_multimodal: false     # Disabled for T4

  # Dropout
  dropout: 0.1

# Training Configuration - T4 Ultra Optimized
training:
  batch_size: 2      # Very small batch for T4 (reduced from 4)
  learning_rate: 0.0001
  warmup_steps: 500
  max_steps: 5000    # Short training for testing
  gradient_accumulation_steps: 16  # High accumulation (increased)
  gradient_clip: 1.0

  # Optimizer
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 0.00000001
  weight_decay: 0.01

  # Mixed Precision - Critical for T4
  use_amp: true
  amp_dtype: "float16"

  # Gradient Checkpointing - Essential for T4
  use_gradient_checkpointing: true

  # Memory Optimization
  max_memory_mb: 12000  # Leave 4GB for system

  # Logging - Frequent for debugging
  log_interval: 25
  save_interval: 250
  eval_interval: 100

# Data Configuration - T4 Optimized
data:
  train_path: "data/train.txt"
  val_path: "data/val.txt"
  max_seq_length: 256
  num_workers: 2  # Reduced for T4
  pin_memory: true
  prefetch_factor: 2

# Hardware - T4 Specific
hardware:
  device: "cuda"
  distributed: false
  num_gpus: 1
  mixed_precision: true
  allow_tf32: false  # T4 doesn't support TF32

# Memory Optimization
memory_optimization:
  use_cpu_offload: false
  use_attention_slicing: true
  use_memory_efficient_attention: true
  max_split_size_mb: 512
  # CUDA memory configuration
  cuda_memory_config: "expandable_segments:True"

# Debugging
debug:
  profile_memory: true
  log_gpu_stats: true
  check_nan: true
