"""
Professional Training Script for AGIFORMER
*** UPDATED WITH TypeError FIX FOR GradScaler ***
"""

import torch, torch.nn as nn, yaml, argparse, json, os, sys, logging
from torch.utils.data import Dataset, DataLoader, random_split
from pathlib import Path
from typing import Dict, Any, Optional, Tuple

sys.path.insert(0, str(Path(__file__).parent))
from agiformer import AGIFORMER
from agiformer.utils import count_parameters, format_number, WarmupScheduler
from scripts.prepare_cc12m import CC12MDataset

try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False

# (CheckpointManager, MetricsLogger, SimpleTextDataset sÄ±nÄ±flarÄ± aynÄ± kalabilir)
class CheckpointManager:
    def __init__(self, checkpoint_dir: str, keep_last_n: int = 5):
        self.checkpoint_dir = Path(checkpoint_dir); self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.keep_last_n = keep_last_n; self.best_val_loss = float('inf')
        logging.basicConfig(level=logging.INFO); self.logger = logging.getLogger(__name__)
    def save_checkpoint(self, step: int, model: nn.Module, optimizer: torch.optim.Optimizer, scheduler: Optional[Any], metrics: Dict[str, float], is_best: bool = False) -> str:
        checkpoint_path = self.checkpoint_dir / f"checkpoint_{step:08d}.pt"
        checkpoint = {'step': step, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict() if scheduler else None, 'metrics': metrics}
        torch.save(checkpoint, checkpoint_path); self.logger.info(f"Checkpoint saved to {checkpoint_path}")
        torch.save(checkpoint, self.checkpoint_dir / "latest.pt")
        if is_best:
            torch.save(checkpoint, self.checkpoint_dir / "best_model.pt"); self.logger.info(f"Best model saved to {self.checkpoint_dir / 'best_model.pt'}")
        self._cleanup_old_checkpoints()
        return str(checkpoint_path)
    def load_checkpoint(self, checkpoint_path: str) -> Dict[str, Any]:
        checkpoint = torch.load(Path(checkpoint_path), map_location='cpu'); self.logger.info(f"Checkpoint loaded from {checkpoint_path}"); return checkpoint
    def _cleanup_old_checkpoints(self):
        checkpoints = sorted(list(self.checkpoint_dir.glob("checkpoint_*.pt")), key=lambda x: x.stat().st_mtime, reverse=True)
        for old in checkpoints[self.keep_last_n:]: old.unlink(); self.logger.info(f"Removed old checkpoint: {old}")

class MetricsLogger:
    def __init__(self, project_name: str, experiment_name: str, config: Dict[str, Any]):
        if WANDB_AVAILABLE and config.get('use_wandb', False):
            wandb.init(project=project_name, name=experiment_name, config=config); self.logger = wandb
        else: self.logger = logging.getLogger(__name__); logging.basicConfig(level=logging.INFO)
    def log_training_metrics(self, step: int, loss: float, model_info: Dict[str, Any]):
        metrics = {'Training/loss': loss, 'Training/step': step}
        if isinstance(self.logger, logging.Logger): self.logger.info(f"Step {step}: {metrics}")
        else: self.logger.log(metrics)
    def log_validation_metrics(self, step: int, val_loss: float, val_metrics: Dict[str, float]):
        metrics = {'Validation/loss': val_loss, 'Validation/step': step, **val_metrics}
        if isinstance(self.logger, logging.Logger): self.logger.info(f"Validation Step {step}: {metrics}")
        else: self.logger.log(metrics)
    def finish(self):
        if hasattr(self.logger, 'finish'): self.logger.finish()

class SimpleTextDataset(Dataset):
    def __init__(self, texts, max_seq_len=512, vocab_size=256):
        self.texts, self.max_seq_len, self.vocab_size = texts, max_seq_len, vocab_size
    def __len__(self): return len(self.texts)
    def __getitem__(self, idx):
        char_ids = [ord(c) % self.vocab_size for c in self.texts[idx][:self.max_seq_len]]
        padding = [0] * (self.max_seq_len - len(char_ids)); char_ids.extend(padding)
        input_ids = torch.tensor(char_ids[:-1], dtype=torch.long); target_ids = torch.tensor(char_ids[1:], dtype=torch.long)
        return input_ids, target_ids

def prepare_batch_data(batch, device, is_multimodal):
    if is_multimodal:
        return {'text': batch['input_ids'].to(device), 'image': batch['image'].to(device)}, batch['target_ids'].to(device)
    else:
        input_ids, target_ids = batch; return {'text': input_ids.to(device)}, target_ids.to(device)

def validate_epoch(model, dataloader, criterion, device, use_amp, is_multimodal):
    model.eval(); total_loss = 0
    with torch.no_grad():
        for batch in dataloader:
            model_inputs, target_ids = prepare_batch_data(batch, device, is_multimodal)
            # --- DEÄžÄ°ÅžÄ°KLÄ°K: 'device_type' burada gerekli ---
            with torch.amp.autocast(device_type=device.type, enabled=use_amp):
                logits, _ = model(**model_inputs)
                loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))
            total_loss += loss.item()
    return total_loss / len(dataloader)

def train_epoch(model, dataloader, optimizer, criterion, device, use_amp, metrics_logger, step_offset, is_multimodal):
    model.train(); total_loss = 0
    # --- DEÄžÄ°ÅžÄ°KLÄ°K: 'device_type' argÃ¼manÄ± kaldÄ±rÄ±ldÄ± ---
    scaler = torch.amp.GradScaler(enabled=use_amp)

    for batch_idx, batch in enumerate(dataloader):
        model_inputs, target_ids = prepare_batch_data(batch, device, is_multimodal)

        # --- DEÄžÄ°ÅžÄ°KLÄ°K: 'device_type' burada gerekli ---
        with torch.amp.autocast(device_type=device.type, enabled=use_amp):
            logits, info = model(**model_inputs)
            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))
            total_loss_batch = loss
            for block_info in info.get('blocks', []):
                if 'moe' in block_info and 'load_balancing_loss' in block_info['moe']['router_info']:
                    total_loss_batch += block_info['moe']['router_info']['load_balancing_loss']

        optimizer.zero_grad()
        scaler.scale(total_loss_batch).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()
        current_step = step_offset + batch_idx
        if metrics_logger and batch_idx % 10 == 0: metrics_logger.log_training_metrics(current_step, loss.item(), info)
        if batch_idx % 50 == 0:
            if torch.cuda.is_available(): print(f"ðŸ“Š GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB used")
            print(f"ðŸš€ Batch {batch_idx}, Loss: {loss.item():.4f}, Step: {current_step}")

    return total_loss / len(dataloader), current_step + 1

def create_dataset(data_dir, data_path, model_config, train_split) -> Tuple[Dataset, Dataset, bool]:
    # (Bu fonksiyonun iÃ§eriÄŸi Ã¶nceki dÃ¼zeltmeyle aynÄ± kalÄ±r)
    if data_dir and Path(data_dir).exists():
        train_metadata, val_metadata = Path(data_dir)/"metadata_train.json", Path(data_dir)/"metadata_val.json"
        if train_metadata.exists() and val_metadata.exists():
            print("Found CC12M dataset structure")
            train_ds = CC12MDataset(data_dir, "train", max_text_len=model_config['max_seq_len'], vocab_size=model_config['vocab_size'])
            val_ds = CC12MDataset(data_dir, "val", max_text_len=model_config['max_seq_len'], vocab_size=model_config['vocab_size'])
            return train_ds, val_ds, True

    print("Using fallback text-only dataset")
    dummy_texts = [
        "This is a sample text for training.", "The quick brown fox jumps over the lazy dog.",
        "Machine learning is fascinating.", "Deep learning models require large datasets.",
        "Natural language processing is evolving rapidly.", "Transformers revolutionized AI research.",
        "Attention mechanisms are powerful.", "AGIFORMER represents the future of AI."
    ] * 1000
    dataset = SimpleTextDataset(dummy_texts, max_seq_len=model_config['max_seq_len'], vocab_size=model_config['vocab_size'])
    train_size = int(train_split * len(dataset)); val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    return train_dataset, val_dataset, False

# (main fonksiyonunun geri kalanÄ± aynÄ±)
def main():
    if 'PYTORCH_CUDA_ALLOC_CONF' not in os.environ:
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        print("ðŸ”§ Set PYTORCH_CUDA_ALLOC_CONF for better memory management")

    parser = argparse.ArgumentParser(description="AGIFORMER Training Script")
    parser.add_argument("--config", type=str, default="configs/base_config.yaml")
    args, unknown_args = parser.parse_known_args()

    with open(args.config, 'r') as f: config = yaml.safe_load(f)

    i = 0
    while i < len(unknown_args):
        arg, val_str = unknown_args[i].strip('--'), unknown_args[i+1]
        try:
            val = int(val_str) if '.' not in val_str else float(val_str)
        except ValueError:
            val = True if val_str.lower() == 'true' else False if val_str.lower() == 'false' else val_str
        keys = arg.split('.'); d = config
        for key in keys[:-1]: d = d.setdefault(key, {})
        d[keys[-1]] = val
        i += 2

    print("Updated config:", json.dumps(config, indent=2))
    model_config, train_config = config['model'], config['training']
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'); print(f"Using device: {device}")

    checkpoint_manager = CheckpointManager(checkpoint_dir="checkpoints")
    metrics_logger = MetricsLogger(project_name="agiformer", experiment_name=config.get('experiment_name', 'agiformer_run'), config=config)

    model = AGIFORMER(
        use_gradient_checkpointing=train_config.get('use_gradient_checkpointing', False),
        **model_config
    ).to(device)

    params = count_parameters(model)
    print(f"\nModel Parameters: Total: {format_number(params['total'])}, Trainable: {format_number(params['trainable'])}")

    train_ds, val_ds, is_multimodal = create_dataset(config.get('data_dir'), config.get('data_path'), model_config, 0.9)
    train_loader = DataLoader(train_ds, batch_size=train_config['batch_size'], shuffle=True, num_workers=0)
    val_loader = DataLoader(val_ds, batch_size=train_config['batch_size'], shuffle=False, num_workers=0)

    optimizer = torch.optim.AdamW(model.parameters(), lr=train_config['learning_rate'], weight_decay=train_config['weight_decay'])
    scheduler = WarmupScheduler(optimizer, warmup_steps=train_config['warmup_steps'], d_model=model_config['d_model'])
    criterion = nn.CrossEntropyLoss(ignore_index=0)

    start_step, best_val_loss = 0, float('inf')
    print(f"\nðŸ”¥ Training Started! Batch size: {train_config['batch_size']}, LR: {train_config['learning_rate']}, AMP: {train_config['use_amp']}")

    epochs = train_config.get('epochs', 10)
    max_steps = train_config.get('max_steps')
    global_step = 0

    try:
        for epoch in range(epochs):
            if max_steps and global_step >= max_steps:
                print(f"Reached max_steps ({max_steps}). Stopping training.")
                break

            print(f"\nðŸ“… Epoch {epoch + 1}/{epochs}")
            avg_loss, end_step = train_epoch(model, train_loader, optimizer, criterion, device, train_config['use_amp'], metrics_logger, global_step, is_multimodal)
            global_step = end_step

            val_loss = validate_epoch(model, val_loader, criterion, device, train_config['use_amp'], is_multimodal)
            scheduler.step()
            if metrics_logger: metrics_logger.log_validation_metrics(end_step, val_loss, {})

            is_best = val_loss < best_val_loss
            if is_best: best_val_loss = val_loss
            checkpoint_manager.save_checkpoint(end_step, model, optimizer, scheduler, {'val_loss': val_loss}, is_best)

            print(f"Epoch {epoch + 1} completed: Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}")

    except KeyboardInterrupt:
        print("\nTraining interrupted by user.")
    finally:
        if metrics_logger: metrics_logger.finish()
        print(f"\nTraining completed! Best validation loss: {best_val_loss:.4f}")

if __name__ == "__main__":
    main()
