# Developer: inkbytefo
# Modified: 2025-11-06

# @package _group_

# AGIFORMER T4 GPU Hardware Configuration
# Settings optimized for NVIDIA T4 GPU training

defaults:
  - _self_

# Device settings
device: cuda
pytorch_cuda_alloc_conf: "max_split_size_mb:128,expandable_segments:True"

# Memory and performance settings
use_amp: true
amp_dtype: float16
num_workers: 2
pin_memory: true
persistent_workers: true

# Batch size optimization for T4
training:
  batch_size: 2  # Reduced from 4 to prevent CUDA OOM (verified working)
  gradient_accumulation_steps: 8
  use_gradient_checkpointing: true
  max_grad_norm: 0.5  # Reduce gradient norm to prevent memory spikes

# T4-specific model settings
model:
  d_model: 384  # Reduced from 512
  n_layers: 6   # Reduced from 8
  n_heads: 6    # Reduced from 8
  d_ff: 1536    # Reduced from 2048
  max_seq_len: 256  # Reduced from 512
  use_memory: true
  use_introspection: false  # Disabled for T4
  use_multimodal: false     # Disabled for T4
  use_linear_attention: false