
# Base training configuration for AGIFORMER
# Matches arguments expected by train.main()(train.py:237)

# Optimization
optimizer: adamw
learning_rate: 3e-4
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-8

# Schedule
warmup_steps: 4000

# Core loop
epochs: 10
batch_size: 8
max_steps: 0            # 0 or null => no explicit max_steps bound
use_amp: true
use_gradient_checkpointing: false

# Checkpointing
checkpoint_dir: checkpoints
keep_last_n_checkpoints: 5
save_interval: 1000     # global_step interval for saving
eval_interval: 500      # global_step interval for validation
log_interval: 50        # global_step interval for logging

# Regularization / stability
max_grad_norm: 1.0
