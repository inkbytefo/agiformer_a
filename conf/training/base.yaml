# Developer: inkbytefo
# Modified: 2025-11-06

# @package _group_

# AGIFORMER Base Training Configuration
# Default training settings

# Batch and Epochs
batch_size: 8
epochs: 10
max_steps: 10000
gradient_accumulation_steps: 4

# Learning Rate
learning_rate: 0.0001
warmup_steps: 1000
lr_scheduler: cosine

# Optimizer
optimizer: adamw
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 0.00000001
weight_decay: 0.01

# Mixed Precision
use_amp: true
amp_dtype: float16

# Gradient Control
gradient_clip: 1.0
use_gradient_checkpointing: true

# Logging and Checkpointing
log_interval: 50
save_interval: 500
eval_interval: 200
checkpoint_dir: checkpoints
keep_last_n_checkpoints: 5

# Early Stopping
early_stopping_patience: 10
early_stopping_metric: val_loss