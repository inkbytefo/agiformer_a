# Baseline Configuration
# Standard Transformer with MultiHeadAttention for comparison with AGIFORMER-Lite
# This configuration is identical to agiformer-lite except for attention mechanism

# Architecture - Same as agiformer-lite for fair comparison
d_model: 512
n_layers: 6
n_heads: 8
d_ff: 2048

# Vocabulary - Same settings
vocab_size: 32000

# Mixture of Experts - Same settings
n_experts: 1
expert_types: ["language"]

# Memory System - Disabled for fair comparison
memory_size: 0
max_seq_len: 512

# Regularization - Same settings
dropout: 0.1

# Feature flags - All disabled for clean comparison
use_linear_attention: false
use_memory: false
use_introspection: false
use_multimodal: false

# Key difference: Use standard MultiHeadAttention instead of AgglutinativeAttention
use_agglutinative_attention: false
morphological_analysis: false

# Training settings - Identical to agiformer-lite
batch_size: 32
learning_rate: 0.0001
warmup_steps: 1000
max_steps: 10000

# Evaluation - Same settings
eval_steps: 500
logging_steps: 100
save_steps: 1000