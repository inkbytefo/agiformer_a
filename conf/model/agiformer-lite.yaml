# AGIFORMER-Lite Configuration
# Simplified version focused on language processing for Phase 1 experiments
# This configuration removes all advanced features to focus on core AgglutinativeAttention comparison

# Architecture - Reduced size for faster training
d_model: 512
n_layers: 6
n_heads: 8
d_ff: 2048

# Vocabulary - Basic byte-pair encoding
vocab_size: 32000

# Mixture of Experts - Only Language Expert for Phase 1
n_experts: 1
expert_types: ["language"]

# Memory System - Disabled for Phase 1
memory_size: 0
max_seq_len: 512

# Regularization
dropout: 0.1

# Feature flags - All disabled for clean comparison
use_linear_attention: false
use_memory: false
use_introspection: false
use_multimodal: false

# Language-specific settings
use_agglutinative_attention: true
morphological_analysis: true

# Training settings for Phase 1
batch_size: 32
learning_rate: 0.0001
warmup_steps: 1000
max_steps: 10000

# Evaluation
eval_steps: 500
logging_steps: 100
save_steps: 1000