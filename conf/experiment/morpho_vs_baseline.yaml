# Developer: inkbytefo
# Modified: 2025-11-07
#
# @package _global_
#
# Experiment: Morphological + Semantic Enrichment vs Baseline (Turkish LM)
#
# Purpose:
#   Empirically measure how much morpho-semantic embeddings
#   (use_morpho_semantic_embeddings: true via LanguageExpert)
#   improve Turkish language modeling (perplexity)
#   compared to a standard Transformer-style baseline.
#
# Design:
#   - This config is a DRIVER that defines TWO runs via overrides:
#       1) Baseline:  no morpho/semantic enrichment
#       2) Morpho:    morpho+semantic enrichment enabled
#   - Each run is executed by selecting the corresponding experiment:
#       experiment=phase1_baseline_morpho_off
#       experiment=phase1_baseline_morpho_on
#
# NOTE:
#   The actual per-variant definitions live in:
#     - conf/experiment/phase1_baseline.yaml        (baseline, morpho OFF)
#     - conf/experiment/phase1_lite.yaml            (can be used as another small baseline)
#   and the updated variants below.

defaults:
  - _self_
  - model: base
  - training: base
  - hardware: default_gpu
  - logging: base

# Common settings for this comparison
experiment_name: morpho_vs_baseline
run_name: ${experiment_name}_${now:%Y%m%d_%H%M%S}

# Data:
# Point this to your Turkish JSONL with:
#   tokens, morpho_types, semantic_categories
data:
  data_dir: null
  data_path: turkish_dataset.jsonl
  val_data_path: null
  train_split: 0.95
  num_workers: 4
  pin_memory: true
  persistent_workers: true

# --------------------------------------------
# VARIANT DEFINITIONS (to be referenced via experiment=...)
# --------------------------------------------

# 1) Baseline: standard Transformer-like behavior, no morpho/semantic enrichment
baseline:
  model:
    # Use same backbone size for fair comparison
    d_model: 512
    n_layers: 8
    n_heads: 8
    d_ff: 2048
    n_experts: 1
    expert_types: ["language"]
    use_agglutinative_attention: false
    # Morphological analysis disabled => LanguageExpert behaves like standard MHA stack
    morphological_analysis: false
  training:
    batch_size: 16
    learning_rate: 0.0003
    max_steps: 20000
    eval_interval: 1000
    log_interval: 100
    save_interval: 5000
    use_gradient_checkpointing: true

# 2) Morpho-Semantic: identical backbone, but with morpho+semantic features ON
morpho_semantic:
  model:
    d_model: 512
    n_layers: 8
    n_heads: 8
    d_ff: 2048
    n_experts: 1
    expert_types: ["language"]
    use_agglutinative_attention: true
    # Enable full pipeline: Turkish-specific morpho/semantic fields are consumed
    morphological_analysis: true
  training:
    batch_size: 16
    learning_rate: 0.0003
    max_steps: 20000
    eval_interval: 1000
    log_interval: 100
    save_interval: 5000
    use_gradient_checkpointing: true

# Usage (from CLI):
#
# Baseline (no morpho/semantic enrichment):
#   python train.py experiment=phase1_baseline
#
# Morpho+Semantic enriched:
#   python train.py experiment=phase1_lite
#
# For a strict A/B with identical backbone, use:
#   python train.py experiment=morpho_vs_baseline baseline
#   python train.py experiment=morpho_vs_baseline morpho_semantic
#
# Then compare:
#   - Validation/main_loss
#   - Perplexity = exp(Validation/main_loss)
#
# This isolates the effect of morpho+semantic embeddings on Turkish LM performance.