# Developer: inkbytefo
# Modified: 2025-11-06

# @package _group_

# AGIFORMER Phase 1 Baseline Experiment Configuration
# Standard Transformer with MultiHeadAttention for comparison with AGIFORMER-Lite

defaults:
  - _self_

# Override model settings for Phase 1 Baseline
model:
  d_model: 256  # Further reduced for T4
  n_layers: 3   # Further reduced for memory
  n_heads: 4    # Further reduced
  d_ff: 1024    # Further reduced
  n_experts: 1
  expert_types: ["language"]
  memory_size: 0
  max_seq_len: 128  # Further reduced
  use_linear_attention: false
  use_memory: false
  use_introspection: false
  use_multimodal: false
  use_agglutinative_attention: false
  morphological_analysis: false

# Override training settings for Phase 1
training:
  batch_size: 2   # Further reduced for T4 memory (overridden for CUDA OOM fix)
  learning_rate: 0.0001
  warmup_steps: 1000
  max_steps: 10000
  epochs: 10
  eval_interval: 500
  log_interval: 100
  save_interval: 1000
  gradient_accumulation_steps: 8  # Further increase to compensate for smaller batch size
  use_gradient_checkpointing: true  # Explicitly enabled to prevent CUDA OOM

# Experiment metadata
experiment_name: phase1_baseline
run_name: phase1_baseline_${now:%Y%m%d_%H%M%S}